# Kie.ai API Scraper

Автоматический сборщик всех моделей Kie.ai с полной документацией API.

## 🚀 Возможности

- ✅ **Параллельная обработка** - многопоточный парсинг для ускорения (ThreadPoolExecutor)
- ✅ **Retry механизм** - автоматические повторы при ошибках (429, 500, 502, 503, 504)
- ✅ **Кэширование** - избежание повторных запросов для ускорения
- ✅ **Метрики производительности** - отслеживание времени, запросов, кэша
- ✅ **Прогресс-бар** - визуализация прогресса обработки в реальном времени
- ✅ **Статистика** - детальная статистика по категориям и параметрам
- ✅ **Экспорт статистики** - сохранение метрик в отдельный JSON файл
- ✅ **Экспорт по категориям** - разделение моделей по категориям в отдельные файлы
- ✅ **Фильтрация моделей** - фильтрация по категории, endpoint, параметрам
- ✅ **Улучшенное извлечение API endpoints** - множественные стратегии поиска
- ✅ **Точное извлечение input параметров** - схемы данных с валидацией
- ✅ **Автоматическое исправление** - исправление проблем в моделях
- ✅ **Валидация структуры** - проверка всех моделей с детальным отчетом
- ✅ **Детальные ответы** - ответы на каждое действие (без молчания)

## 📦 Установка

```bash
pip install -r requirements.txt
```

## 🚀 Deploy to Render

**Service type:** Background Worker (polling)  
**Branch:** `main`  
**Auto Deploy:** ON

**Build Command:**
```bash
pip install -r requirements.txt
```

**Start Command:**
```bash
python main_render.py
```

**Required ENV:**
- `TELEGRAM_BOT_TOKEN`
- `KIE_API_KEY`
- `KIE_BASE_URL`
- `DATABASE_URL`

**Optional ENV:**
- `PORT=10000` (healthcheck listener)
- `DRY_RUN=0`

### Конфигурация (опционально)

Создайте `config.json` на основе `config.json.example`:

```json
{
  "base_url": "https://api.kie.ai/api/v1",
  "market_url": "https://kie.ai/ru/market",
  "max_models": 50,
  "request_delay": 0.3,
  "timeout": 15
}
```

Или используйте переменные окружения:
- `KIE_BASE_URL` - базовый URL API
- `KIE_MARKET_URL` - URL маркета
- `MAX_MODELS` - максимальное количество моделей
- `REQUEST_DELAY` - задержка между запросами
- `REQUEST_TIMEOUT` - таймаут запросов

## 🎯 Использование

### ⚠️ Важно: Одноразовый парсинг

Парсинг выполняется **один раз локально**. Результаты сохраняются в `kie_full_api.json` и используются на Render без повторного парсинга.

```bash
# Запуск парсинга (один раз)
python kie_api_scraper.py

# Принудительный перезапуск (если данные уже есть)
python kie_api_scraper.py --force
```

Скрипт автоматически:
1. Сканирует страницу маркета
2. Парсит документацию каждой модели (параллельно)
3. Валидирует структуру всех моделей
4. Сохраняет результаты в `kie_full_api.json`
5. Сохраняет статистику в `kie_scraper_stats.json`

**После парсинга:**
- Закоммитьте `kie_full_api.json` в Git
- На Render парсинг **НЕ запускается** автоматически
- Используются уже спарсенные данные

### Дополнительные опции (как библиотека):

```python
from kie_api_scraper import KieApiScraper

# Настройка количества потоков и кэширования
scraper = KieApiScraper(max_workers=10, enable_cache=True)

# Запуск парсинга
models = scraper.run_full_scrape()

# Фильтрация моделей
video_models = scraper.filter_models(category='video', has_endpoint=True)

# Экспорт по категориям
scraper.export_models_by_category('exports')
```

### Переменные окружения:

- `EXPORT_BY_CATEGORY=true` - включить экспорт по категориям

## 📋 Структура данных

Каждая модель содержит:
- `name` - название модели
- `endpoint` - API endpoint (проверен и валидирован)
- `method` - HTTP метод (обычно POST)
- `base_url` - базовый URL API
- `params` - параметры модели (duration, width, height, steps, temperature, max_length)
- `input_schema` - схема входных данных с обязательными полями
- `example` - пример использования (JSON строка)
- `example_request` - структурированный пример запроса (объект)
- `price` - цена (если доступна)
- `category` - категория модели (video, image, text, audio, other)

## ✅ Валидация

Скрипт автоматически проверяет:
- Наличие всех обязательных полей
- Правильность типов данных
- Соответствие base_url
- Структуру параметров

## 📁 Файлы

- `kie_api_scraper.py` - основной скрипт
- `requirements.txt` - зависимости Python
- `runtime.txt` - версия Python для Render
- `render.yaml` - конфигурация для деплоя на Render
- `.renderignore` - игнорируемые файлы
- `kie_full_api.json` - ⭐ **ГЛАВНЫЙ ФАЙЛ** - результат парсинга (создается после локального запуска)
- `kie_scraper_stats.json` - статистика и метрики (создается после запуска)
- `kie_scraper.log` - лог файл с детальной информацией
- `config.json` - конфигурация (опционально, см. config.json.example)
- `exports/` - экспорт по категориям (если включен)

**Важно:** Файл `kie_full_api.json` должен быть закоммичен в Git и использоваться на Render без повторного парсинга.

## 🔧 Требования

- Python 3.7+
- requests>=2.31.0
- beautifulsoup4>=4.12.0
- lxml>=4.9.0
- urllib3>=2.0.0

## ⚡ Производительность

- **Параллельная обработка**: до 5-10 потоков (настраивается)
- **Кэширование**: избежание повторных запросов
- **Retry механизм**: автоматические повторы при ошибках
- **Оптимизированный парсинг**: множественные стратегии поиска

## 📊 Метрики

После выполнения создается файл `kie_scraper_stats.json` с:
- Временем выполнения
- Количеством запросов (всего, кэшированных, ошибок)
- Статистикой по категориям
- Результатами валидации

## 📝 Лицензия

MIT
